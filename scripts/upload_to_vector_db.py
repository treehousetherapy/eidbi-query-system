# eidbi-query-system/scripts/upload_to_vector_db.py

import logging
import json
import argparse
import os
from typing import List, Dict, Any

# --- Path Setup & Settings Import ---
try:
    import sys
    SCRIPT_DIR = os.path.dirname(__file__)
    PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
    sys.path.append(PROJECT_ROOT)
    # Add scraper utils path for vertex_ai_utils if needed later
    # SCRAPER_UTILS_PATH = os.path.join(PROJECT_ROOT, 'scraper', 'utils')
    # if SCRAPER_UTILS_PATH not in sys.path:
    #     sys.path.append(SCRAPER_UTILS_PATH)
    from config.settings import settings
    # Import vector DB service ONLY if direct upsert were feasible/implemented
    # from backend.app.services.vector_db_service import upsert_embeddings 
except ImportError as e:
    print(f"Error importing modules in upload_to_vector_db.py: {e}")
    print("Ensure config/settings.py exists relative to the project root.")
    exit(1)
# --- End Import ---

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s')
logger = logging.getLogger(__name__)

def load_chunks_from_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """Loads chunk data from a JSON Lines file."""
    chunks = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        chunk = json.loads(line)
                        if 'id' in chunk and 'embedding' in chunk:
                            chunks.append(chunk)
                        else:
                            logger.warning(f"Skipping line in {file_path} due to missing 'id' or 'embedding': {line.strip()[:100]}...")
                    except json.JSONDecodeError:
                        logger.warning(f"Skipping invalid JSON line in {file_path}: {line.strip()[:100]}...")
    except FileNotFoundError:
        logger.error(f"Input file not found: {file_path}")
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}", exc_info=True)
    return chunks

def main(input_jsonl_path: str, index_id: str):
    """Main function to load data and prepare for upsert (placeholder)."""
    logger.info(f"Loading chunks from: {input_jsonl_path}")
    chunks_data = load_chunks_from_jsonl(input_jsonl_path)

    if not chunks_data:
        logger.error("No valid chunks with IDs and embeddings found in the input file. Exiting.")
        return

    logger.info(f"Loaded {len(chunks_data)} chunks with embeddings.")

    # Prepare datapoints in the format expected by Matching Engine (ID, Embedding)
    datapoints = [
        (chunk['id'], chunk['embedding'])
        for chunk in chunks_data
    ]

    logger.warning("--- Placeholder for Vector DB Upsert --- ")
    logger.warning("Matching Engine Index updates are typically done via batch update jobs.")
    logger.warning("This script currently only loads data and shows what would be upserted.")
    logger.info(f"Target Index ID: {index_id}")
    logger.info(f"Number of datapoints prepared for upsert: {len(datapoints)}")

    # --- Placeholder for calling an upsert function or generating batch files ---
    # Option 1: Call direct upsert (if implemented and suitable)
    # success = upsert_embeddings(datapoints=datapoints, index_id_override=index_id)
    # if success:
    #      logger.info("Upsert operation submitted successfully (actual update is asynchronous).")
    # else:
    #      logger.error("Upsert operation failed.")

    # Option 2: Generate JSON file(s) for batch update
    # You would need to format datapoints according to:
    # https://cloud.google.com/vertex-ai/docs/matching-engine/update-rebuild-index#console
    # and upload these files to GCS, then trigger a batch update via gcloud or console.
    # Example formatting:
    # batch_update_data = [
    #     {"id": dp_id, "embedding": vector} for dp_id, vector in datapoints
    # ]
    # batch_filename = f"batch_update_{os.path.basename(input_jsonl_path)}"
    # with open(batch_filename, 'w', encoding='utf-8') as f_batch:
    #     for item in batch_update_data:
    #         json.dump(item, f_batch)
    #         f_batch.write('\n')
    # logger.info(f"Generated batch update file: {batch_filename}")
    # logger.info("Upload this file to GCS and use `gcloud ai indexes update ...`")
    # --------------------------------------------------------------------------

    logger.info("Script finished (Placeholder Upsert).")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Load embeddings from JSONL and prepare for Vector DB upsert.")
    parser.add_argument("input_file", help="Path to the input JSON Lines file generated by the scraper (containing IDs and embeddings).")
    parser.add_argument("-i", "--index-id", help="Target Vertex AI Matching Engine Index ID (overrides config).", default=settings.vector_db.index_id)

    args = parser.parse_args()

    if not args.index_id:
        logger.error("Error: Vector DB Index ID is required. Provide via --index-id argument or set VECTOR_DB_INDEX_ID in .env")
    elif not os.path.exists(args.input_file):
        logger.error(f"Error: Input file not found: {args.input_file}")
    else:
        main(input_jsonl_path=args.input_file, index_id=args.index_id) 